{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils.autoaugment import CIFAR10Policy\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bayesian_config as cf\n",
    "\n",
    "from utils.BBBlayers import GaussianVariationalInference\n",
    "from utils.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
    "from utils.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
    "from utils.BayesianModels.BayesianLeNet import BBBLeNet\n",
    "from utils.BayesianModels.BayesianSqueezeNet import BBBSqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Bayesian Model Training')\n",
    "#parser.add_argument('--lr', default=0.001, type=float, help='learning_rate')\n",
    "parser.add_argument('--net_type', default='lenet', type=str, help='model')\n",
    "#parser.add_argument('--depth', default=28, type=int, help='depth of model')\n",
    "#parser.add_argument('--widen_factor', default=10, type=int, help='width of model')\n",
    "#parser.add_argument('--num_samples', default=10, type=int, help='Number of samples')\n",
    "#parser.add_argument('--beta_type', default=\"Blundell\", type=str, help='Beta type')\n",
    "#parser.add_argument('--p_logvar_init', default=0, type=int, help='p_logvar_init')\n",
    "#parser.add_argument('--q_logvar_init', default=-10, type=int, help='q_logvar_init')\n",
    "#parser.add_argument('--weight_decay', default=0.0005, type=float, help='weight_decay')\n",
    "parser.add_argument('--dataset', default='CIFAR10', type=str, help='dataset = [MNIST/CIFAR10/CIFAR100]')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "parser.add_argument('--testOnly', '-t', action='store_true', help='Test mode with the saved model')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameter settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "best_acc = 0\n",
    "resize=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channels = ['red channel', 'green channel', 'blue channel']\n",
    "\n",
    "fig = plt.figure(figsize = (36, 36)) \n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', size=8,\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return network & file name\n",
    "def getNetwork(args):\n",
    "    if (args.net_type == 'lenet'):\n",
    "        net = BBBLeNet(outputs,inputs)\n",
    "        file_name = 'lenet'\n",
    "    elif (args.net_type == 'alexnet'):\n",
    "        net = BBBAlexNet(outputs,inputs)\n",
    "        file_name = 'alexnet-'\n",
    "    elif (args.net_type == '3conv3fc'):\n",
    "        net = BBB3Conv3FC(outputs,inputs)\n",
    "        file_name = '3Conv3FC-'\n",
    "    else:\n",
    "        print('Error : Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
    "        sys.exit(0)\n",
    "\n",
    "    return net, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "print('\\n[Phase 2] : Model setup')\n",
    "if args.resume:\n",
    "    # Load checkpoint\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    _, file_name = getNetwork(args)\n",
    "    checkpoint = torch.load('./checkpoint/'+args.dataset+os.sep+file_name+str(cf.num_samples)+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    cf.start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('| Building net type [' + args.net_type + ']...')\n",
    "    net, file_name = getNetwork(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vi = GaussianVariationalInference(torch.nn.CrossEntropyLoss())\n",
    "optimizer = optim.Adam(net.parameters(), lr=cf.learning_rate(cf.lr, epoch), weight_decay=cf.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logfile = os.path.join('diagnostics_Bayes{}_{}_{}.txt'.format(args.net_type, args.dataset, cf.num_samples))\n",
    "value_file = os.path.join(\"values{}_{}.txt\".format(args.net_type, args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    m = math.ceil(len(trainset) / cf.batch_size)\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        \n",
    "        data = data.view(-1, inputs, resize, resize).repeat(cf.num_samples, 1, 1, 1)\n",
    "        target = target.repeat(cf.num_samples)\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        if cf.beta_type is \"Blundell\":\n",
    "            beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        elif cf.beta_type is \"Soenderby\":\n",
    "            beta = min(epoch / (cf.num_epochs // 4), 1)\n",
    "        elif cf.beta_type is \"Standard\":\n",
    "            beta = 1 / m\n",
    "        else:\n",
    "            beta = 0\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output,kl = net.probforward(data)\n",
    "        # calculate the batch loss\n",
    "        loss = vi(output, target, kl, beta)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output,kl = net(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target, kl, beta)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(net.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "    diagnostics_to_write =  {'Epoch': epoch, 'Loss': loss.data[0], 'Accuracy': (100*correct/total)/cf.num_samples}\n",
    "    with open(logfile, 'a') as lf:\n",
    "        lf.write(str(diagnostics_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "net.eval()\n",
    "m = math.ceil(len(testset) / cf.batch_size)\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    \n",
    "    data = data.view(-1, inputs, resize, resize).repeat(cf.num_samples, 1, 1, 1)\n",
    "    target = target.repeat(cf.num_samples)\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    if cf.beta_type is \"Blundell\":\n",
    "            beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        elif cf.beta_type is \"Soenderby\":\n",
    "            beta = min(epoch / (cf.num_epochs // 4), 1)\n",
    "        elif cf.beta_type is \"Standard\":\n",
    "            beta = 1 / m\n",
    "        else:\n",
    "            beta = 0\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output, kl = net.probforward(data)\n",
    "    # calculate the batch loss\n",
    "    loss = vi(output, target, kl, beta)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()\n",
    "\n",
    "# move model inputs to cuda, if GPU available\n",
    "if train_on_gpu:\n",
    "    images = images.cuda()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
